"""
This type stub file was generated by pyright.
"""

import os
import sys
import platform
import textwrap
import ctypes
import torch._C as _C
import torch.cuda
import torch.autograd
import torch.futures
import torch.nn
import torch.nn.intrinsic
import torch.nn.quantized
import torch.optim
import torch.optim._multi_tensor
import torch.multiprocessing
import torch.sparse
import torch.utils.backcompat
import torch.onnx
import torch.jit
import torch.linalg
import torch.hub
import torch.random
import torch.distributions
import torch.testing
import torch.backends.cuda
import torch.backends.mkl
import torch.backends.mkldnn
import torch.backends.openmp
import torch.backends.quantized
import torch.quantization
import torch.utils.data
import torch.__config__
import torch.__future__
import torch.quasirandom
from ._utils import _import_dotted_name
from ._utils_internal import USE_GLOBAL_DEPS, USE_RTLD_GLOBAL_WITH_LIBTORCH, get_file_path, prepare_multiprocessing_environment
from .version import __version__
from ._six import string_classes as _string_classes
from typing import Set, TYPE_CHECKING, Type
from torch._C import *
from .tensor import Tensor
from .storage import _StorageBase
from .random import get_rng_state, initial_seed, manual_seed, seed, set_rng_state
from .serialization import load, save
from ._tensor_str import set_printoptions
from torch._C._VariableFunctions import *
from .functional import *
from torch.autograd import enable_grad, no_grad, set_grad_enabled
from . import _storage_docs, _tensor_docs, _torch_docs
from torch._ops import ops
from torch._classes import classes
from torch.multiprocessing._atfork import register_after_fork
from ._lobpcg import lobpcg

"""
This type stub file was generated by pyright.
"""
if sys.version_info < (3, ):
    ...
if sys.platform == 'win32':
    ...
if USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv('TORCH_USE_RTLD_GLOBAL') and platform.system() != 'Windows':
    old_flags = sys.getdlopenflags()
else:
    ...
if TYPE_CHECKING:
    ...
def typename(o):
    ...

def is_tensor(obj):
    r"""Returns True if `obj` is a PyTorch tensor.

    Note that this function is simply doing ``isinstance(obj, Tensor)``.
    Using that ``isinstance`` check is better for typechecking with mypy,
    and more explicit - so it's recommended to use that instead of
    ``is_tensor``.

    Args:
        obj (Object): Object to test
    """
    ...

def is_storage(obj):
    r"""Returns True if `obj` is a PyTorch storage object.

    Args:
        obj (Object): Object to test
    """
    ...

def set_default_tensor_type(t):
    r"""Sets the default ``torch.Tensor`` type to floating point tensor type
    ``t``. This type will also be used as default floating point type for
    type inference in :func:`torch.tensor`.

    The default floating point tensor type is initially ``torch.FloatTensor``.

    Args:
        t (type or string): the floating point tensor type or its name

    Example::

        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32
        torch.float32
        >>> torch.set_default_tensor_type(torch.DoubleTensor)
        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor
        torch.float64

    """
    ...

def set_default_dtype(d):
    r"""Sets the default floating point dtype to :attr:`d`.
    This dtype is:

    1. The inferred dtype for python floats in :func:`torch.tensor`.
    2. Used to infer dtype for python complex numbers. The default complex dtype is set to
       ``torch.complex128`` if default floating point dtype is ``torch.float64``,
       otherwise it's set to ``torch.complex64``

    The default floating point dtype is initially ``torch.float32``.

    Args:
        d (:class:`torch.dtype`): the floating point dtype to make the default

    Example:
        >>> # initial default for floating point is torch.float32
        >>> torch.tensor([1.2, 3]).dtype
        torch.float32
        >>> # initial default for floating point is torch.complex64
        >>> torch.tensor([1.2, 3j]).dtype
        torch.complex64
        >>> torch.set_default_dtype(torch.float64)
        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor
        torch.float64
        >>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor
        torch.complex128

    """
    ...

def set_deterministic(d):
    r""" Sets whether PyTorch operations must use "deterministic"
    algorithms. That is, algorithms which, given the same input, and when
    run on the same software and hardware, always produce the same output.
    When True, operations will use deterministic algorithms when available,
    and if only nondeterministic algorithms are available they will throw a
    :class:RuntimeError when called.

    .. warning::
        This feature is in beta, and its design and implementation may change
        in the future.

    The following normally-nondeterministic operations will act
    deterministically when `d=True`:

        * :class:`torch.nn.Conv1d` when called on CUDA tensor
        * :class:`torch.nn.Conv2d` when called on CUDA tensor
        * :class:`torch.nn.Conv3d` when called on CUDA tensor
        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor
        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor
        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor
        * :func:`torch.bmm` when called on sparse-dense CUDA tensors

    The following normally-nondeterministic operations will throw a
    :class:`RuntimeError` when `d=True`:

        * :class:`torch.nn.AvgPool3d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.AdaptiveAvgPool2d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.AdaptiveAvgPool3d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.MaxPool3d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.AdaptiveMaxPool2d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.FractionalMaxPool2d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.FractionalMaxPool3d` when called on a CUDA tensor that requires grad
        * :func:`torch.nn.functional.interpolate` when called on a CUDA tensor that requires grad
            and one of the following modes is used:
            - `linear`
            - `bilinear`
            - `bicubic`
            - `trilinear`
        * :class:`torch.nn.ReflectionPad1d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.ReflectionPad2d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.ReplicationPad1d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.ReplicationPad2d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.ReplicationPad3d` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.CTCLoss` when called on a CUDA tensor that requires grad
        * :class:`torch.nn.EmbeddingBag` when called on a CUDA tensor that requires grad
        * :func:`torch.scatter_add_` when called on a CUDA tensor
        * :func:`torch.index_add_` when called on a CUDA tensor
        * :func:`torch.index_select` when called on a CUDA tensor that requires grad
        * :func:`torch.repeat_interleave` when called on a CUDA tensor that requires grad
        * :func:`torch.histc` when called on a CUDA tensor
        * :func:`torch.bincount` when called on a CUDA tensor

    A handful of CUDA operations are nondeterministic if the CUDA version is
    10.2 or greater, unless the environment variable `CUBLAS_WORKSPACE_CONFIG=:4096:8`
    or `CUBLAS_WORKSPACE_CONFIG=:16:8` is set. See the CUDA documentation for more
    details: `<https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility>`_
    If one of these environment variable configurations is not set, a :class:`RuntimeError`
    will be raised from these operations when called with CUDA tensors:

        * :func:`torch.mm`
        * :func:`torch.mv`
        * :func:`torch.bmm`

    Note that deterministic operations tend to have worse performance than
    non-deterministic operations.

    Args:
        d (:class:`bool`): If True, force operations to be deterministic.
                           If False, allow non-deterministic operations.
    """
    ...

def is_deterministic():
    r"""Returns True if the global deterministic flag is turned on. Refer to
    :func:`torch.set_deterministic` documentation for more details.
    """
    ...

class DoubleStorage(_C.DoubleStorageBase, _StorageBase):
    ...


class FloatStorage(_C.FloatStorageBase, _StorageBase):
    ...


class HalfStorage(_C.HalfStorageBase, _StorageBase):
    ...


class LongStorage(_C.LongStorageBase, _StorageBase):
    ...


class IntStorage(_C.IntStorageBase, _StorageBase):
    ...


class ShortStorage(_C.ShortStorageBase, _StorageBase):
    ...


class CharStorage(_C.CharStorageBase, _StorageBase):
    ...


class ByteStorage(_C.ByteStorageBase, _StorageBase):
    ...


class BoolStorage(_C.BoolStorageBase, _StorageBase):
    ...


class BFloat16Storage(_C.BFloat16StorageBase, _StorageBase):
    ...


class ComplexDoubleStorage(_C.ComplexDoubleStorageBase, _StorageBase):
    ...


class ComplexFloatStorage(_C.ComplexFloatStorageBase, _StorageBase):
    ...


class QUInt8Storage(_C.QUInt8StorageBase, _StorageBase):
    ...


class QInt8Storage(_C.QInt8StorageBase, _StorageBase):
    ...


class QInt32Storage(_C.QInt32StorageBase, _StorageBase):
    ...


_storage_classes = (DoubleStorage, FloatStorage, LongStorage, IntStorage, ShortStorage, CharStorage, ByteStorage, HalfStorage, BoolStorage, QUInt8Storage, QInt8Storage, QInt32Storage, BFloat16Storage, ComplexFloatStorage, ComplexDoubleStorage)
_tensor_classes: Set[Type] = set()
def manager_path():
    ...

if TYPE_CHECKING:
    ...
def compiled_with_cxx11_abi():
    r"""Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"""
    ...

legacy_contiguous_format = contiguous_format
quantized_lstm = torch.ops.aten.quantized_lstm
quantized_gru = torch.ops.aten.quantized_gru
def Assert(condition, message):
    r"""A wrapper around Python's assert which is symbolically traceable.
    """
    ...

