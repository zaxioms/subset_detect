"""
This type stub file was generated by pyright.
"""

import contextlib
import os
import torch
import traceback
import warnings
import threading
import torch._C
from typing import List, Optional, Tuple, Union
from ._utils import _dummy_type, _get_device_index
from .streams import Event, Stream
from .. import device as _device
from .memory import *
from .random import *
from ..storage import _StorageBase
from . import amp, nvtx, profiler, sparse

"""
This type stub file was generated by pyright.
"""
_initialized = False
_tls = threading.local()
_initialization_lock = threading.Lock()
_queued_calls = []
_is_in_bad_fork = getattr(torch._C, "_cuda_isInBadFork", lambda : False)
_device_t = Union[_device, str, int]
if hasattr(torch._C, '_CudaDeviceProperties'):
    _CudaDeviceProperties = torch._C._CudaDeviceProperties
else:
    _CudaDeviceProperties = _dummy_type('_CudaDeviceProperties')
has_magma: bool = False
has_half: bool = False
default_generators: Tuple[torch._C.Generator] = ()
def is_available() -> bool:
    r"""Returns a bool indicating if CUDA is currently available."""
    ...

def is_initialized():
    r"""Returns whether PyTorch's CUDA state has been initialized."""
    ...

class DeferredCudaCallError(Exception):
    ...


def init():
    r"""Initialize PyTorch's CUDA state.  You may need to call
    this explicitly if you are interacting with PyTorch via
    its C API, as Python bindings for CUDA functionality will not
    be available until this initialization takes place.  Ordinary users
    should not need this, as all of PyTorch's CUDA methods
    automatically initialize CUDA state on-demand.

    Does nothing if the CUDA state is already initialized.
    """
    ...

def cudart():
    ...

class cudaStatus(object):
    SUCCESS: int = ...
    ERROR_NOT_READY: int = ...


class CudaError(RuntimeError):
    def __init__(self, code: int) -> None:
        ...
    


def check_error(res: int) -> None:
    ...

class device(object):
    r"""Context-manager that changes the selected device.

    Arguments:
        device (torch.device or int): device index to select. It's a no-op if
            this argument is a negative integer or ``None``.
    """
    def __init__(self, device) -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, *args):
        ...
    


class device_of(device):
    r"""Context-manager that changes the current device to that of given object.

    You can use both tensors and storages as arguments. If a given object is
    not allocated on a GPU, this is a no-op.

    Arguments:
        obj (Tensor or Storage): object allocated on the selected device.
    """
    def __init__(self, obj) -> None:
        ...
    


def set_device(device: _device_t) -> None:
    r"""Sets the current device.

    Usage of this function is discouraged in favor of :any:`device`. In most
    cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.

    Arguments:
        device (torch.device or int): selected device. This function is a no-op
            if this argument is negative.
    """
    ...

def get_device_name(device: Optional[_device_t] = ...) -> str:
    r"""Gets the name of a device.

    Arguments:
        device (torch.device or int, optional): device for which to return the
            name. This function is a no-op if this argument is a negative
            integer. It uses the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).
    """
    ...

def get_device_capability(device: Optional[_device_t] = ...) -> Tuple[int, int]:
    r"""Gets the cuda capability of a device.

    Arguments:
        device (torch.device or int, optional): device for which to return the
            device capability. This function is a no-op if this argument is
            a negative integer. It uses the current device, given by
            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
            (default).

    Returns:
        tuple(int, int): the major and minor cuda capability of the device
    """
    ...

def get_device_properties(device: _device_t) -> _CudaDeviceProperties:
    ...

@contextlib.contextmanager
def stream(stream):
    r"""Context-manager that selects a given stream.

    All CUDA kernels queued within its context will be enqueued on a selected
    stream.

    Arguments:
        stream (Stream): selected stream. This manager is a no-op if it's
            ``None``.

    .. note:: Streams are per-device. If the selected stream is not on the
        current device, this function will also change the current device to
        match the stream.
    """
    ...

def device_count() -> int:
    r"""Returns the number of GPUs available."""
    ...

def get_arch_list() -> List[str]:
    r"""Returns list CUDA architectures this library was compiled for."""
    ...

def get_gencode_flags() -> str:
    r"""Returns NVCC gencode flags this library were compiled with."""
    ...

def current_device() -> int:
    r"""Returns the index of a currently selected device."""
    ...

def synchronize(device: _device_t = ...) -> None:
    r"""Waits for all kernels in all streams on a CUDA device to complete.

    Arguments:
        device (torch.device or int, optional): device for which to synchronize.
            It uses the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).
    """
    ...

def ipc_collect():
    r"""Force collects GPU memory after it has been released by CUDA IPC.

    .. note::
        Checks if any sent CUDA tensors could be cleaned from the memory. Force
        closes shared memory file used for reference counting if there is no
        active counters. Useful when the producer process stopped actively sending
        tensors and want to release unused memory.
    """
    ...

def current_stream(device: Optional[_device_t] = ...) -> Stream:
    r"""Returns the currently selected :class:`Stream` for a given device.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            the currently selected :class:`Stream` for the current device, given
            by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
            (default).
    """
    ...

def default_stream(device: Optional[_device_t] = ...) -> Stream:
    r"""Returns the default :class:`Stream` for a given device.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            the default :class:`Stream` for the current device, given by
            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
            (default).
    """
    ...

def current_blas_handle():
    r"""Returns cublasHandle_t pointer to current cuBLAS handle"""
    ...

if not hasattr(torch._C, 'CudaDoubleStorageBase'):
    ...
class _CudaBase(object):
    is_cuda = ...
    is_sparse = ...
    def type(self, *args, **kwargs):
        ...
    
    __new__ = ...


class DoubleStorage(_CudaBase, torch._C.CudaDoubleStorageBase, _StorageBase):
    ...


class FloatStorage(_CudaBase, torch._C.CudaFloatStorageBase, _StorageBase):
    ...


class LongStorage(_CudaBase, torch._C.CudaLongStorageBase, _StorageBase):
    ...


class IntStorage(_CudaBase, torch._C.CudaIntStorageBase, _StorageBase):
    ...


class ShortStorage(_CudaBase, torch._C.CudaShortStorageBase, _StorageBase):
    ...


class CharStorage(_CudaBase, torch._C.CudaCharStorageBase, _StorageBase):
    ...


class ByteStorage(_CudaBase, torch._C.CudaByteStorageBase, _StorageBase):
    ...


class HalfStorage(_CudaBase, torch._C.CudaHalfStorageBase, _StorageBase):
    ...


class BoolStorage(_CudaBase, torch._C.CudaBoolStorageBase, _StorageBase):
    ...


class BFloat16Storage(_CudaBase, torch._C.CudaBFloat16StorageBase, _StorageBase):
    ...


class ComplexDoubleStorage(_CudaBase, torch._C.CudaComplexDoubleStorageBase, _StorageBase):
    ...


class ComplexFloatStorage(_CudaBase, torch._C.CudaComplexFloatStorageBase, _StorageBase):
    ...


