"""
This type stub file was generated by pyright.
"""

from torch.nn.modules.utils import _pair, _single, _triple
from torch.onnx.symbolic_helper import parse_args

def unused(g):
    ...

def reshape(g, self, shape):
    ...

def reshape_as(g, self, other):
    ...

def add(g, self, other, alpha=...):
    ...

def sub(g, self, other, alpha=...):
    ...

def rsub(g, self, other, alpha=...):
    ...

def mul(g, self, other):
    ...

def div(g, self, other):
    ...

def floor_divide(g, self, other):
    ...

def floordiv(g, self, other):
    ...

def true_divide(g, self, other):
    ...

def reciprocal(g, self):
    ...

@parse_args('v', 'i')
def cat(g, tensor_list, dim):
    ...

@parse_args('v', 'i')
def stack(g, tensor_list, dim):
    ...

def mm(g, self, other):
    ...

def bmm(g, self, other):
    ...

def matmul(g, self, other):
    ...

@parse_args('v', 'v', 'v', 't', 't')
def addmm(g, self, mat1, mat2, beta, alpha):
    ...

def neg(g, self):
    ...

def sqrt(g, self):
    ...

def rsqrt(g, self):
    ...

def tanh(g, self):
    ...

def sin(g, self):
    ...

def cos(g, self):
    ...

def tan(g, self):
    ...

def asin(g, self):
    ...

def acos(g, self):
    ...

def atan(g, self):
    ...

def sigmoid(g, self):
    ...

def sign(g, self):
    ...

def overload_by_arg_count(fn):
    ...

sum = _reduce_with_dtype('ReduceSum', 'sum')
mean = _reduce_with_dtype('ReduceMean', 'mean')
prod = _reduce_with_dtype('ReduceProd', 'prod', allow_multi_dim_support=False)
@parse_args('v', 'i', 'none')
def cumsum(g, input, dim, dtype):
    ...

def t(g, self):
    ...

def expand(g, self, size, implicit):
    ...

def expand_as(g, self, other):
    ...

def embedding(g, weight, indices, padding_idx, scale_grad_by_freq, sparse):
    ...

@parse_args('v', 'v', 'v', 'i', 'i', 'i', 'v', 'i')
def embedding_bag(g, embedding_matrix, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset):
    ...

def size(g, self, dim=...):
    ...

@parse_args('v', 'i', 'i')
def transpose(g, self, dim0, dim1):
    ...

@parse_args('v', 'is')
def permute(g, self, dims):
    ...

def view(g, self, size):
    ...

def view_as(g, self, other):
    ...

def prim_ConstantSplit(g, self, split_size, dim):
    ...

def prim_ConstantChunk(g, self, chunks, dim):
    ...

@parse_args('v', 'i', 'i', 'i')
def unsafe_chunk(g, self, chunks, dim, _outputs=...):
    ...

@parse_args('v', 'v', 'v', 'i')
def split(g, self, split_size_or_sizes, dim, _outputs=...):
    ...

def unsafe_split(g, self, split_size_or_sizes, dim, _outputs=...):
    ...

@parse_args('v', 'is', 'i', 'i')
def split_with_sizes(g, self, split_sizes, dim, _outputs=...):
    ...

def unsafe_split_with_sizes(g, self, split_sizes, dim, _outputs=...):
    ...

@parse_args('v', 'i', 'i')
def unbind(g, self, dim=..., _outputs=...):
    ...

@parse_args('v', 'i', 'v')
def select(g, self, dim, index):
    ...

def squeeze(g, self, dim=...):
    ...

def prelu(g, self, weight):
    ...

def relu(g, input):
    ...

def ceil(g, input):
    ...

def floor(g, input):
    ...

@parse_args('v', 't', 't')
def threshold(g, self, threshold, value):
    ...

def leaky_relu(g, input, negative_slope, inplace=...):
    ...

@parse_args('v', 'i')
def glu(g, input, dim):
    ...

@parse_args('v', 'i', 'none')
def softmax(g, input, dim, dtype=...):
    ...

@parse_args('v', 't', 'v')
def softplus(g, self, beta, threshold):
    ...

def get_pool_ceil_padding(input, kernel_size, stride, padding):
    ...

max_pool1d = _max_pool("max_pool1d", _single, 1, return_indices=False)
max_pool2d = _max_pool("max_pool2d", _pair, 2, return_indices=False)
max_pool3d = _max_pool("max_pool3d", _triple, 3, return_indices=False)
max_pool1d_with_indices = _max_pool("max_pool1d_with_indices", _single, 1, return_indices=True)
max_pool2d_with_indices = _max_pool("max_pool2d_with_indices", _pair, 2, return_indices=True)
max_pool3d_with_indices = _max_pool("max_pool3d_with_indices", _triple, 3, return_indices=True)
avg_pool1d = _avg_pool('avg_pool1d', _single)
avg_pool2d = _avg_pool('avg_pool2d', _pair)
avg_pool3d = _avg_pool('avg_pool3d', _triple)
adaptive_avg_pool1d = _adaptive_pool('adaptive_avg_pool1d', "AveragePool", _single)
adaptive_avg_pool2d = _adaptive_pool('adaptive_avg_pool2d', "AveragePool", _pair)
adaptive_avg_pool3d = _adaptive_pool('adaptive_avg_pool3d', "AveragePool", _triple)
adaptive_max_pool1d = _adaptive_pool('adaptive_max_pool1d', "MaxPool", _single, max_pool1d_with_indices)
adaptive_max_pool2d = _adaptive_pool('adaptive_max_pool2d', "MaxPool", _pair, max_pool2d_with_indices)
adaptive_max_pool3d = _adaptive_pool('adaptive_max_pool3d', "MaxPool", _triple, max_pool3d_with_indices)
def constant_pad_nd(g, input, padding, value):
    ...

def reflection_pad(g, input, padding):
    ...

def replication_pad(g, input, padding):
    ...

reflection_pad1d = reflection_pad
reflection_pad2d = reflection_pad
reflection_pad3d = reflection_pad
replication_pad1d = replication_pad
replication_pad2d = replication_pad
replication_pad3d = replication_pad
upsample_nearest1d = _interpolate('upsample_nearest1d', 3, "nearest")
upsample_nearest2d = _interpolate('upsample_nearest2d', 4, "nearest")
upsample_nearest3d = _interpolate('upsample_nearest3d', 5, "nearest")
upsample_linear1d = _interpolate('upsample_linear1d', 3, "linear")
upsample_bilinear2d = _interpolate('upsample_bilinear2d', 4, "linear")
upsample_trilinear3d = _interpolate('upsample_trilinear3d', 5, "linear")
@parse_args('v')
def bitwise_not(g, inp):
    ...

def wrap_logical_op_with_cast_to(to_type):
    ...

def wrap_logical_op_with_cast_to_and_from(to_type):
    ...

def wrap_logical_op_with_negation(func):
    ...

def eq(g, self, other):
    ...

@wrap_logical_op_with_negation
def ne(g, self, other):
    ...

def gt(g, input, other):
    ...

def gt_impl(g, input, other):
    ...

def lt(g, input, other):
    ...

def lt_impl(g, input, other):
    ...

@wrap_logical_op_with_negation
def ge(g, input, other):
    ...

@wrap_logical_op_with_negation
def le(g, input, other):
    ...

@parse_args('v', 'v', 'v', 'i')
def where(g, condition, self=..., other=..., _outputs=...):
    ...

@parse_args('v', 'i', 'none')
def log_softmax(g, input, dim, dtype=...):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i')
def conv1d(g, input, weight, bias, stride, padding, dilation, groups):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i')
def conv2d(g, input, weight, bias, stride, padding, dilation, groups):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i')
def conv3d(g, input, weight, bias, stride, padding, dilation, groups):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'is')
def conv_transpose1d(g, input, weight, bias, stride, padding, output_padding, groups, dilation):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'is')
def conv_transpose2d(g, input, weight, bias, stride, padding, output_padding, groups, dilation):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'is')
def conv_transpose3d(g, input, weight, bias, stride, padding, output_padding, groups, dilation):
    ...

@parse_args('v', 'v', 'v', 'v', 'v', 'i', 'f', 'f', 'i')
def batch_norm(g, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled):
    ...

@parse_args('v', 'is', 'v', 'v', 'f', 'i')
def layer_norm(g, input, normalized_shape, weight, bias, eps, cudnn_enable):
    ...

@parse_args('v', 'v', 'v', 'v', 'v', 'i', 'f', 'f', 'i')
def instance_norm(g, input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled):
    ...

@parse_args('v', 'i', 'i', 'i')
def unfold(g, input, dimension, size, step):
    ...

@parse_args('v', 't', 't', 't')
def elu(g, input, alpha, scale, input_scale):
    ...

def selu(g, input):
    ...

@parse_args('v', 'i', 'v')
def index_select(g, self, dim, index):
    ...

def index_put(g, self, indices_list_value, values, accumulate):
    ...

def index_fill(g, self, dim, index, value):
    ...

def index_copy(g, self, dim, index, source):
    ...

def type_as(g, self, other):
    ...

@parse_args('v', 'v', 'i', 'f')
def cosine_similarity(g, x1, x2, dim, eps):
    ...

def clone(g, input, unused_memory_format):
    ...

def abs(g, self):
    ...

def log(g, self):
    ...

def log1p(g, self):
    ...

def pow(g, self, exponent):
    ...

def clamp(g, self, min, max):
    ...

@parse_args('v', 'f')
def clamp_min(g, self, min):
    ...

@parse_args('v', 'f')
def clamp_max(g, self, max):
    ...

def max(g, self, dim_or_y=..., keepdim=...):
    ...

def min(g, self, dim_or_y=..., keepdim=...):
    ...

def exp(g, self):
    ...

@parse_args('v', 'f', 'i')
def dropout(g, input, p, train):
    ...

feature_dropout = _unsupported_dropout("feature_dropout")
alpha_dropout = _unsupported_dropout("alpha_dropout")
feature_alpha_dropout = _unsupported_dropout("feature_alpha_dropout")
dropout_ = dropout
feature_dropout_ = feature_dropout
alpha_dropout_ = alpha_dropout
feature_alpha_dropout_ = feature_alpha_dropout
@parse_args('v', 't', 'is', 'i')
def norm(g, self, p, dim, keepdim):
    ...

@parse_args('v', 'v', 'v', 'i')
def conv_tbc(g, input, weight, bias, pad):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def empty(g, sizes, dtype, layout, device, pin_memory=..., memory_format=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def empty_like(g, input, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def new_empty(g, self, sizes, dtype, layout, device, pin_memory=...):
    ...

def scalar_tensor(g, scalar, dtype, *options):
    ...

def tensor(g, data, dtype=..., device=..., requires_grad=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v')
def zeros(g, sizes, dtype, layout, device, pin_memory=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def zeros_like(g, input, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def new_zeros(g, self, sizes, dtype, layout, device, pin_memory=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v')
def ones(g, sizes, dtype, layout, device, pin_memory=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def ones_like(g, input, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def full(g, sizes, value, dtype, layout, device, pin_memory=...):
    ...

def full_like(g, input, fill_value, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def new_full(g, self, size, fill_value, dtype, layout, device, pin_memory=...):
    ...

def eye(g, n, m, dtype=..., layout=..., device=..., pin_memory=...):
    ...

def slice(g, self, *args):
    ...

@parse_args('v', 'f', 'f')
def hardtanh(g, self, min_val, max_val):
    ...

def alias(g, self):
    ...

@parse_args('v', 'i')
def unsqueeze(g, self, dim):
    ...

@parse_args('v', 'i', 'i', 'none')
def sort(g, self, dim, decending, out=...):
    ...

def numel(g, self):
    ...

@parse_args('v', 'i', 'i', 'i', 'i', 'none')
def topk(g, self, k, dim, largest, sorted, out=...):
    ...

def to(g, self, *args):
    ...

def repeat(g, self, repeats):
    ...

@parse_args('v', 'i')
def pixel_shuffle(g, self, upscale_factor):
    ...

def lstm(g, *args):
    ...

gru = _one_hidden_rnn('GRU')
rnn_tanh = _one_hidden_rnn('RNN_TANH')
rnn_relu = _one_hidden_rnn('RNN_RELU')
def detach(g, input):
    ...

@parse_args('v', 'i')
def contiguous(g, input, memory_format):
    ...

def randn(g, shapes, dtype, *options):
    ...

def rand(g, shapes, dtype, *options):
    ...

def randn_like(g, self, dtype, layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def rand_like(g, self, dtype, layout=..., device=..., pin_memory=..., memory_format=...):
    ...

@parse_args('v', 'f', 'f', 'i', 'none')
def rrelu(g, input, lower, upper, training, generator):
    ...

@parse_args('v')
def log_sigmoid(g, input):
    ...

@parse_args('v')
def erf(g, input):
    ...

@parse_args('v', 'i', 'i')
def flatten(g, input, start_dim, end_dim):
    ...

@parse_args('v')
def nonzero(g, input):
    ...

@parse_args('v')
def isnan(g, input):
    ...

@parse_args('v', 'i', 'i', 'i')
def narrow(g, input, dim, start, length):
    ...

def argmax(g, input, dim, keepdim):
    ...

def argmin(g, input, dim, keepdim):
    ...

@parse_args('v', 'i', 'v', 'v')
def scatter(g, self, dim, index, src):
    ...

@parse_args('v', 'i', 'v', 'v')
def scatter_add(g, self, dim, index, src):
    ...

def log2(g, self):
    ...

def prim_shape(g, self):
    ...

@parse_args('v', 'i')
def one_hot(g, self, num_classes):
    ...

@parse_args('v', 'i', 'v', 'v')
def gather(g, self, dim, index, sparse_grad=...):
    ...

def std(g, input, *args):
    ...

@parse_args('v', 'is', 'i')
def logsumexp(g, input, dim, keepdim):
    ...

def arange(g, *args):
    ...

def masked_fill(g, self, mask, value):
    ...

def index(g, self, index):
    ...

@parse_args('v', 'is', 'i')
def frobenius_norm(g, self, dim=..., keepdim=...):
    ...

@parse_args('v', 'i', 'b', 'v')
def multinomial(g, input, num_samples, replacement=..., generator=...):
    ...

def baddbmm(g, self, batch1, batch2, beta, alpha):
    ...

def meshgrid(g, tensor_list):
    ...

def remainder(g, input, other):
    ...

def gelu(g, self):
    ...

@parse_args('v', 'i', 'v', 'v', 'f', 'i')
def group_norm(g, input, num_groups, weight, bias, eps, cudnn_enabled):
    ...

def dim(g, self):
    '''Implement the dim functionality available for a pytorch tensor in ONNX'''
    ...

def take(g, self, index):
    ...

@parse_args('v', 'v', 'i', 'b')
def kl_div(g, input, target, reduction, log_target):
    ...

@parse_args('v', 'v', 'is', 'i')
def as_strided(g, self, sizes, strides, offset=...):
    ...

