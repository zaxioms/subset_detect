"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from torch import Tensor
from torch._jit_internal import Optional, Tuple
from torch.nn.utils.rnn import PackedSequence

"""
This type stub file was generated by pyright.
"""
def apply_permutation(tensor: Tensor, permutation: Tensor, dim: int = ...) -> Tensor:
    ...

class PackedParameter(torch.nn.Module):
    def __init__(self, param) -> None:
        ...
    


class RNNBase(torch.nn.Module):
    _FLOAT_MODULE = ...
    _version = ...
    def __init__(self, mode, input_size, hidden_size, num_layers=..., bias=..., batch_first=..., dropout=..., bidirectional=..., dtype=...) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    def __repr__(self):
        ...
    
    def check_input(self, input: Tensor, batch_sizes: Optional[Tensor]) -> None:
        ...
    
    def get_expected_hidden_size(self, input: Tensor, batch_sizes: Optional[Tensor]) -> Tuple[int, int, int]:
        ...
    
    def check_hidden_size(self, hx: Tensor, expected_hidden_size: Tuple[int, int, int], msg: str = ...) -> None:
        ...
    
    def check_forward_args(self, input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]) -> None:
        ...
    
    def permute_hidden(self, hx: Tensor, permutation: Optional[Tensor]) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    
    def get_weight(self):
        ...
    
    def get_bias(self):
        ...
    


class LSTM(RNNBase):
    r"""
    A dynamic quantized LSTM module with floating point tensor as inputs and outputs.
    We adopt the same interface as `torch.nn.LSTM`, please see
    https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.

    Examples::

        >>> rnn = nn.LSTM(10, 20, 2)
        >>> input = torch.randn(5, 3, 10)
        >>> h0 = torch.randn(2, 3, 20)
        >>> c0 = torch.randn(2, 3, 20)
        >>> output, (hn, cn) = rnn(input, (h0, c0))
    """
    _FLOAT_MODULE = ...
    __overloads__ = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def forward_impl(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]], batch_sizes: Optional[Tensor], max_batch_size: int, sorted_indices: Optional[Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        ...
    
    @torch.jit.export
    def forward_tensor(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        ...
    
    @torch.jit.export
    def forward_packed(self, input: PackedSequence, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[PackedSequence, Tuple[Tensor, Tensor]]:
        ...
    
    def permute_hidden(self, hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) -> Tuple[Tensor, Tensor]:
        ...
    
    def check_forward_args(self, input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor]) -> None:
        ...
    
    @torch.jit.ignore
    def forward(self, input, hx=...):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class RNNCellBase(torch.nn.Module):
    __constants__ = ...
    def __init__(self, input_size, hidden_size, bias=..., num_chunks=..., dtype=...) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    def check_forward_input(self, input):
        ...
    
    def check_forward_hidden(self, input: Tensor, hx: Tensor, hidden_label: str = ...) -> None:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    
    def get_weight(self):
        ...
    
    def get_bias(self):
        ...
    


class RNNCell(RNNCellBase):
    r"""An Elman RNN cell with tanh or ReLU non-linearity.
    A dynamic quantized RNNCell module with floating point tensor as inputs and outputs.
    Weights are quantized to 8 bits. We adopt the same interface as `torch.nn.RNNCell`,
    please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.

    Examples::

        >>> rnn = nn.RNNCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx = rnn(input[i], hx)
                output.append(hx)
    """
    __constants__ = ...
    def __init__(self, input_size, hidden_size, bias=..., nonlinearity=..., dtype=...) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class LSTMCell(RNNCellBase):
    r"""A long short-term memory (LSTM) cell.

    A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs.
    Weights are quantized to 8 bits. We adopt the same interface as `torch.nn.LSTMCell`,
    please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.

    Examples::

        >>> rnn = nn.LSTMCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> cx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx, cx = rnn(input[i], (hx, cx))
                output.append(hx)
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[Tensor, Tensor]:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class GRUCell(RNNCellBase):
    r"""A gated recurrent unit (GRU) cell

    A dynamic quantized GRUCell module with floating point tensor as inputs and outputs.
    Weights are quantized to 8 bits. We adopt the same interface as `torch.nn.GRUCell`,
    please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.

    Examples::

        >>> rnn = nn.GRUCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx = rnn(input[i], hx)
                output.append(hx)
    """
    def __init__(self, input_size, hidden_size, bias=..., dtype=...) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


