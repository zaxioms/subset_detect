"""
This type stub file was generated by pyright.
"""

import torch
from torch.nn import Module
from .observer import HistogramObserver, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver

"""
This type stub file was generated by pyright.
"""
class FakeQuantize(Module):
    r""" Simulate the quantize and dequantize operations in training time.
    The output of this module is given by

    x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale



    * :attr:`scale` defines the scale factor used for quantization.

    * :attr:`zero_point` specifies the quantized value to which 0 in floating point maps to

    * :attr:`quant_min` specifies the minimum allowable quantized value.

    * :attr:`quant_max` specifies the maximum allowable quantized value.

    * :attr:`fake_quant_enable` controls the application of fake quantization on tensors, note that
      statistics can still be updated.

    * :attr:`observer_enable` controls statistics collection on tensors

    * :attr:`dtype` specifies the quantized dtype that is being emulated with fake-quantization,
                    allowable values are torch.qint8 and torch.quint8. The values of quant_min and
                    quant_max should be chosen to be consistent with the dtype


    Args:
        observer (module): Module for observing statistics on input tensors and calculating scale
                           and zero-point.
        quant_min (int): The minimum allowable quantized value.
        quant_max (int): The maximum allowable quantized value.
        observer_kwargs (optional): Arguments for the observer module

    Attributes:
        observer (Module): User provided module that collects statistics on the input tensor and
                           provides a method to calculate scale and zero-point.

    """
    def __init__(self, observer=..., quant_min=..., quant_max=..., **observer_kwargs) -> None:
        ...
    
    @torch.jit.export
    def enable_fake_quant(self, enabled: bool = ...) -> None:
        ...
    
    @torch.jit.export
    def disable_fake_quant(self):
        ...
    
    @torch.jit.export
    def enable_observer(self, enabled: bool = ...) -> None:
        ...
    
    @torch.jit.export
    def disable_observer(self):
        ...
    
    @torch.jit.export
    def calculate_qparams(self):
        ...
    
    def forward(self, X):
        ...
    
    with_args = ...
    @torch.jit.export
    def extra_repr(self):
        ...
    


default_fake_quant = FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
default_weight_fake_quant = FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=- 128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, reduce_range=False)
default_per_channel_weight_fake_quant = FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver, quant_min=- 128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0)
default_histogram_fake_quant = FakeQuantize.with_args(observer=HistogramObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
def disable_fake_quant(mod):
    ...

def enable_fake_quant(mod):
    ...

def disable_observer(mod):
    ...

def enable_observer(mod):
    ...

