"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.utils import lazy_property

"""
This type stub file was generated by pyright.
"""
class Transform(object):
    """
    Abstract class for invertable transformations with computable log
    det jacobians. They are primarily used in
    :class:`torch.distributions.TransformedDistribution`.

    Caching is useful for transforms whose inverses are either expensive or
    numerically unstable. Note that care must be taken with memoized values
    since the autograd graph may be reversed. For example while the following
    works with or without caching::

        y = t(x)
        t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.

    However the following will error when caching due to dependency reversal::

        y = t(x)
        z = t.inv(y)
        grad(z.sum(), [y])  # error because z is x

    Derived classes should implement one or both of :meth:`_call` or
    :meth:`_inverse`. Derived classes that set `bijective=True` should also
    implement :meth:`log_abs_det_jacobian`.

    Args:
        cache_size (int): Size of cache. If zero, no caching is done. If one,
            the latest single value is cached. Only 0 and 1 are supported.

    Attributes:
        domain (:class:`~torch.distributions.constraints.Constraint`):
            The constraint representing valid inputs to this transform.
        codomain (:class:`~torch.distributions.constraints.Constraint`):
            The constraint representing valid outputs to this transform
            which are inputs to the inverse transform.
        bijective (bool): Whether this transform is bijective. A transform
            ``t`` is bijective iff ``t.inv(t(x)) == x`` and
            ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in
            the codomain. Transforms that are not bijective should at least
            maintain the weaker pseudoinverse properties
            ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.
        sign (int or Tensor): For bijective univariate transforms, this
            should be +1 or -1 depending on whether transform is monotone
            increasing or decreasing.
        event_dim (int): Number of dimensions that are correlated together in
            the transform ``event_shape``. This should be 0 for pointwise
            transforms, 1 for transforms that act jointly on vectors, 2 for
            transforms that act jointly on matrices, etc.
    """
    bijective = ...
    event_dim = ...
    def __init__(self, cache_size=...) -> None:
        ...
    
    @property
    def inv(self):
        """
        Returns the inverse :class:`Transform` of this transform.
        This should satisfy ``t.inv.inv is t``.
        """
        ...
    
    @property
    def sign(self):
        """
        Returns the sign of the determinant of the Jacobian, if applicable.
        In general this only makes sense for bijective transforms.
        """
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def __ne__(self, other) -> bool:
        ...
    
    def __call__(self, x):
        """
        Computes the transform `x => y`.
        """
        ...
    
    def log_abs_det_jacobian(self, x, y):
        """
        Computes the log det jacobian `log |dy/dx|` given input and output.
        """
        ...
    
    def __repr__(self):
        ...
    


class _InverseTransform(Transform):
    """
    Inverts a single :class:`Transform`.
    This class is private; please instead use the ``Transform.inv`` property.
    """
    def __init__(self, transform) -> None:
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    
    @property
    def bijective(self):
        ...
    
    @property
    def sign(self):
        ...
    
    @property
    def event_dim(self):
        ...
    
    @property
    def inv(self):
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def __call__(self, x):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class ComposeTransform(Transform):
    """
    Composes multiple transforms in a chain.
    The transforms being composed are responsible for caching.

    Args:
        parts (list of :class:`Transform`): A list of transforms to compose.
        cache_size (int): Size of cache. If zero, no caching is done. If one,
            the latest single value is cached. Only 0 and 1 are supported.
    """
    def __init__(self, parts, cache_size=...) -> None:
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    
    @lazy_property
    def bijective(self):
        ...
    
    @lazy_property
    def sign(self):
        ...
    
    @lazy_property
    def event_dim(self):
        ...
    
    @property
    def inv(self):
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __call__(self, x):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    
    def __repr__(self):
        ...
    


identity_transform = ComposeTransform([])
class ExpTransform(Transform):
    r"""
    Transform via the mapping :math:`y = \exp(x)`.
    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class PowerTransform(Transform):
    r"""
    Transform via the mapping :math:`y = x^{\text{exponent}}`.
    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    def __init__(self, exponent, cache_size=...) -> None:
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class SigmoidTransform(Transform):
    r"""
    Transform via the mapping :math:`y = \frac{1}{1 + \exp(-x)}` and :math:`x = \text{logit}(y)`.
    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class TanhTransform(Transform):
    r"""
    Transform via the mapping :math:`y = \tanh(x)`.

    It is equivalent to
    ```
    ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])
    ```
    However this might not be numerically stable, thus it is recommended to use `TanhTransform`
    instead.

    Note that one should use `cache_size=1` when it comes to `NaN/Inf` values.

    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class AbsTransform(Transform):
    r"""
    Transform via the mapping :math:`y = |x|`.
    """
    domain = ...
    codomain = ...
    def __eq__(self, other) -> bool:
        ...
    


class AffineTransform(Transform):
    r"""
    Transform via the pointwise affine mapping :math:`y = \text{loc} + \text{scale} \times x`.

    Args:
        loc (Tensor or float): Location parameter.
        scale (Tensor or float): Scale parameter.
        event_dim (int): Optional size of `event_shape`. This should be zero
            for univariate random variables, 1 for distributions over vectors,
            2 for distributions over matrices, etc.
    """
    domain = ...
    codomain = ...
    bijective = ...
    def __init__(self, loc, scale, event_dim=..., cache_size=...) -> None:
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    @property
    def sign(self):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class SoftmaxTransform(Transform):
    r"""
    Transform from unconstrained space to the simplex via :math:`y = \exp(x)` then
    normalizing.

    This is not bijective and cannot be used for HMC. However this acts mostly
    coordinate-wise (except for the final normalization), and thus is
    appropriate for coordinate-wise optimization algorithms.
    """
    domain = ...
    codomain = ...
    event_dim = ...
    def __eq__(self, other) -> bool:
        ...
    


class StickBreakingTransform(Transform):
    """
    Transform from unconstrained space to the simplex of one additional
    dimension via a stick-breaking process.

    This transform arises as an iterated sigmoid transform in a stick-breaking
    construction of the `Dirichlet` distribution: the first logit is
    transformed via sigmoid to the first probability and the probability of
    everything else, and then the process recurses.

    This is bijective and appropriate for use in HMC; however it mixes
    coordinates together and is less appropriate for optimization.
    """
    domain = ...
    codomain = ...
    bijective = ...
    event_dim = ...
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class LowerCholeskyTransform(Transform):
    """
    Transform from unconstrained matrices to lower-triangular matrices with
    nonnegative diagonal entries.

    This is useful for parameterizing positive definite matrices in terms of
    their Cholesky factorization.
    """
    domain = ...
    codomain = ...
    event_dim = ...
    def __eq__(self, other) -> bool:
        ...
    


class CatTransform(Transform):
    """
    Transform functor that applies a sequence of transforms `tseq`
    component-wise to each submatrix at `dim`, of length `lengths[dim]`,
    in a way compatible with :func:`torch.cat`.

    Example::
       x0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)
       x = torch.cat([x0, x0], dim=0)
       t0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])
       t = CatTransform([t0, t0], dim=0, lengths=[20, 20])
       y = t(x)
    """
    def __init__(self, tseq, dim=..., lengths=..., cache_size=...) -> None:
        ...
    
    @lazy_property
    def length(self):
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    
    @property
    def bijective(self):
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    


class StackTransform(Transform):
    """
    Transform functor that applies a sequence of transforms `tseq`
    component-wise to each submatrix at `dim`
    in a way compatible with :func:`torch.stack`.

    Example::
       x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)
       t = StackTransform([ExpTransform(), identity_transform], dim=1)
       y = t(x)
    """
    def __init__(self, tseq, dim=..., cache_size=...) -> None:
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    
    @property
    def bijective(self):
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    


